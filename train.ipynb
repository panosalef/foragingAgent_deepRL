{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c7ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from Agent import *\n",
    "from Model import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080f635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.gamma_shape = 10    # defines Gamma dist.\n",
    "        self.gamma_mus = np.array([3, 6, 9])\n",
    "        self.batch_size = arg.batch_size\n",
    "        \n",
    "    def reset(self):\n",
    "        # have a nonexistent 4th box option, the action pushing it means not choose any box\n",
    "        self.box_time_count = np.zeros((self.batch_size, self.gamma_mus.size + 1))  \n",
    "        self.box_mus = np.ones((self.batch_size, self.gamma_mus.size))\n",
    "        self.box_sampled_interval = np.ones_like(self.box_time_count) * 10000\n",
    "        for trial_idx in range(self.batch_size):\n",
    "            self.box_mus[trial_idx] = np.random.permutation(self.gamma_mus)    # randomize mus for three boxes\n",
    "        \n",
    "        # assign randomized mus for the first three box options, the 4th option has a mu of 10000 meaning nothing will happen\n",
    "        # after choosing it, i.e., not choose any of the three boxes\n",
    "        self.box_sampled_interval[:, :self.gamma_mus.size] = np.round(np.random.gamma(shape=self.gamma_shape, \n",
    "                                                                      scale=self.box_mus / self.gamma_shape)).clip(1, 10000)\n",
    "        # Sample a std of observation noise from the range [0, 1)\n",
    "        self.obs_std = np.random.rand(self.batch_size, 1) * 1\n",
    "        \n",
    "    def get_obs(self):\n",
    "        # observation, a ratio of box time count to the total box reward interval, 1 means reward is available\n",
    "        obs = (self.box_time_count / self.box_sampled_interval)[:, :self.gamma_mus.size].clip(0, 1)\n",
    "        # add observation noise\n",
    "        obs += obs * np.random.randn(*obs.shape) * self.obs_std\n",
    "        # provide extra observation uncertainty as agent input\n",
    "        obs = np.concatenate([obs, self.obs_std], axis=-1)\n",
    "        return obs\n",
    "        \n",
    "    def push_button(self, box_idx):\n",
    "        # give reward after pushing, reward = 1 if reward is available otherwise 0\n",
    "        reward = self.box_time_count[np.arange(box_idx.size), box_idx] >= \\\n",
    "                 self.box_sampled_interval[np.arange(box_idx.size), box_idx]\n",
    "        reward = reward.astype(float)\n",
    "        \n",
    "        # cost of pushing a button. Pushing the 4th option, i.e., not pushing any box, has no cost\n",
    "        cost = -np.ones_like(reward) * 0.01\n",
    "        cost[box_idx == 3] = 0\n",
    "        \n",
    "        #reward_shaped = reward.copy()\n",
    "        #time_diff = self.box_time_count[np.arange(box_idx.size), box_idx] - \\\n",
    "        #            self.box_sampled_interval[np.arange(box_idx.size), box_idx]\n",
    "        #reward_shaped[(time_diff > 0) & (time_diff < 10)] = 1 - (time_diff[(time_diff > 0) & (time_diff < 10)] / 20)\n",
    "        #reward_shaped[time_diff >= 10] = 0.5\n",
    "        #reward_shaped[(time_diff > -10) & (time_diff < 0)] = time_diff[(time_diff > -10) & (time_diff < 0)] / 20\n",
    "        #reward_shaped[time_diff < -10] = -0.5\n",
    "        #reward_shaped[box_idx != 3] = 0\n",
    "        \n",
    "        # time step update. Reset box timer after pushing. Resample box reward interval.\n",
    "        self.box_time_count += 1 \n",
    "        self.box_time_count[np.arange(box_idx.size), box_idx] = 0\n",
    "        self.box_sampled_interval[box_idx != 3, box_idx[box_idx != 3]] = np.round(\n",
    "                                np.random.gamma(shape=self.gamma_shape, \n",
    "                                                scale=self.box_mus[box_idx != 3, box_idx[box_idx != 3]]\n",
    "                                                / self.gamma_shape)).clip(1, 10000)\n",
    "        return reward+cost, reward+cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c80805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "seed_number = 2\n",
    "datapath = Path(rf'C:\\Users\\Panos\\OneDrive - nyu.edu\\Documents\\PhD Classes\\Machine Learning\\ML_Project\\Code\\seed{seed_number}')\n",
    "arg = config.ConfigCore(datapath)\n",
    "arg.SEED_NUMBER = seed_number\n",
    "arg.save()\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(seed_number)\n",
    "torch.cuda.manual_seed_all(seed_number)\n",
    "np.random.seed(seed_number)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d615ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "agent = Agent(arg, ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5634e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi 0, reward_avg  177.330, reward_num  88665, reward_frac  0.177, entropy 1.385\n",
      "epi 1, reward_avg  181.420, reward_num  90710, reward_frac  0.181, entropy 1.377\n",
      "epi 2, reward_avg  190.140, reward_num  95070, reward_frac  0.190, entropy 1.35\n",
      "epi 3, reward_avg  202.938, reward_num  101469, reward_frac  0.203, entropy 1.307\n",
      "epi 4, reward_avg  216.467, reward_num  108234, reward_frac  0.216, entropy 1.244\n",
      "epi 5, reward_avg  229.560, reward_num  114780, reward_frac  0.230, entropy 1.16\n",
      "epi 6, reward_avg  245.090, reward_num  122545, reward_frac  0.245, entropy 1.065\n",
      "epi 7, reward_avg  258.137, reward_num  129068, reward_frac  0.258, entropy 0.999\n",
      "epi 8, reward_avg  268.704, reward_num  134352, reward_frac  0.269, entropy 0.887\n",
      "epi 9, reward_avg  282.699, reward_num  141349, reward_frac  0.283, entropy 0.813\n",
      "epi 10, reward_avg  291.293, reward_num  145646, reward_frac  0.291, entropy 0.709\n",
      "epi 11, reward_avg  303.393, reward_num  151697, reward_frac  0.303, entropy 0.682\n",
      "epi 12, reward_avg  308.464, reward_num  154232, reward_frac  0.308, entropy 0.65\n",
      "epi 13, reward_avg  309.609, reward_num  154804, reward_frac  0.310, entropy 0.574\n",
      "epi 14, reward_avg  315.994, reward_num  157997, reward_frac  0.316, entropy 0.55\n",
      "epi 15, reward_avg  318.524, reward_num  159262, reward_frac  0.319, entropy 0.489\n",
      "epi 16, reward_avg  322.639, reward_num  161320, reward_frac  0.323, entropy 0.457\n",
      "epi 17, reward_avg  328.154, reward_num  164077, reward_frac  0.328, entropy 0.434\n",
      "epi 18, reward_avg  330.850, reward_num  165425, reward_frac  0.331, entropy 0.404\n",
      "epi 19, reward_avg  333.576, reward_num  166788, reward_frac  0.334, entropy 0.379\n",
      "epi 20, reward_avg  336.908, reward_num  168454, reward_frac  0.337, entropy 0.351\n",
      "epi 21, reward_avg  336.900, reward_num  168450, reward_frac  0.337, entropy 0.354\n",
      "epi 22, reward_avg  335.680, reward_num  167840, reward_frac  0.336, entropy 0.335\n",
      "epi 23, reward_avg  337.104, reward_num  168552, reward_frac  0.337, entropy 0.324\n",
      "epi 24, reward_avg  341.138, reward_num  170569, reward_frac  0.341, entropy 0.315\n",
      "epi 25, reward_avg  341.667, reward_num  170834, reward_frac  0.342, entropy 0.302\n",
      "epi 26, reward_avg  344.213, reward_num  172106, reward_frac  0.344, entropy 0.294\n",
      "epi 27, reward_avg  342.024, reward_num  171012, reward_frac  0.342, entropy 0.284\n",
      "epi 28, reward_avg  340.364, reward_num  170182, reward_frac  0.340, entropy 0.278\n",
      "epi 29, reward_avg  343.716, reward_num  171858, reward_frac  0.344, entropy 0.274\n",
      "epi 30, reward_avg  342.483, reward_num  171242, reward_frac  0.342, entropy 0.268\n",
      "epi 31, reward_avg  346.797, reward_num  173398, reward_frac  0.347, entropy 0.266\n",
      "epi 32, reward_avg  340.748, reward_num  170374, reward_frac  0.341, entropy 0.258\n",
      "epi 33, reward_avg  346.214, reward_num  173107, reward_frac  0.346, entropy 0.257\n",
      "epi 34, reward_avg  346.414, reward_num  173207, reward_frac  0.346, entropy 0.256\n",
      "epi 35, reward_avg  346.107, reward_num  173054, reward_frac  0.346, entropy 0.249\n",
      "epi 36, reward_avg  343.691, reward_num  171845, reward_frac  0.344, entropy 0.242\n",
      "epi 37, reward_avg  346.174, reward_num  173087, reward_frac  0.346, entropy 0.24\n",
      "epi 38, reward_avg  345.031, reward_num  172516, reward_frac  0.345, entropy 0.237\n",
      "epi 39, reward_avg  349.880, reward_num  174940, reward_frac  0.350, entropy 0.235\n",
      "epi 40, reward_avg  346.459, reward_num  173229, reward_frac  0.346, entropy 0.231\n",
      "epi 41, reward_avg  347.558, reward_num  173779, reward_frac  0.348, entropy 0.228\n",
      "epi 42, reward_avg  343.842, reward_num  171921, reward_frac  0.344, entropy 0.224\n",
      "epi 43, reward_avg  346.152, reward_num  173076, reward_frac  0.346, entropy 0.221\n",
      "epi 44, reward_avg  347.040, reward_num  173520, reward_frac  0.347, entropy 0.219\n",
      "epi 45, reward_avg  349.082, reward_num  174541, reward_frac  0.349, entropy 0.217\n",
      "epi 46, reward_avg  343.961, reward_num  171980, reward_frac  0.344, entropy 0.213\n",
      "epi 47, reward_avg  347.425, reward_num  173712, reward_frac  0.347, entropy 0.212\n",
      "epi 48, reward_avg  346.518, reward_num  173259, reward_frac  0.347, entropy 0.209\n",
      "epi 49, reward_avg  349.164, reward_num  174582, reward_frac  0.349, entropy 0.205\n",
      "epi 50, reward_avg  344.179, reward_num  172089, reward_frac  0.344, entropy 0.202\n",
      "epi 51, reward_avg  345.914, reward_num  172957, reward_frac  0.346, entropy 0.202\n",
      "epi 52, reward_avg  350.478, reward_num  175239, reward_frac  0.350, entropy 0.2\n",
      "epi 53, reward_avg  348.406, reward_num  174203, reward_frac  0.348, entropy 0.196\n",
      "epi 54, reward_avg  345.542, reward_num  172771, reward_frac  0.346, entropy 0.194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(rewards); agent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(rewards)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# update model\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     59\u001b[0m agent\u001b[38;5;241m.\u001b[39msave(i_epi)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - nyu.edu\\Documents\\PhD Classes\\Machine Learning\\ML_Project\\Code\\Agent.py:103\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 103\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "File \u001b[1;32m~\\OneDrive - nyu.edu\\Documents\\PhD Classes\\Machine Learning\\ML_Project\\Code\\Agent.py:81\u001b[0m, in \u001b[0;36mAgent.update_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m advantages, values_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGAE(rewards, dones, values_current_old, values_next_old)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epoch):                \n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m#actionlogprobs, values_current, dist_entropy = self.model(states, actions_old, hidden_in=[actorhidden0s, critichidden0s])\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     actionlogprobs, values_current, dist_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mactorhidden0s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactorhidden0s\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     policy_ratios \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(actionlogprobs \u001b[38;5;241m-\u001b[39m actionlogprobs_old)\n\u001b[0;32m     84\u001b[0m     surr1 \u001b[38;5;241m=\u001b[39m policy_ratios \u001b[38;5;241m*\u001b[39m advantages\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - nyu.edu\\Documents\\PhD Classes\\Machine Learning\\ML_Project\\Code\\Model.py:120\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x, action, hidden_in)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, action, hidden_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# actor\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     a, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_in\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_dist(a)\n\u001b[0;32m    122\u001b[0m     action_logprob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# total training trials, usually converge around 100-200 trials\n",
    "num_trial = 201\n",
    "\n",
    "reward_log = reward_num = done_num = 0\n",
    "agent.buffer.clear()\n",
    "\n",
    "def get_init_variables():   \n",
    "    state = torch.zeros(1, arg.batch_size, 7, device=arg.device)\n",
    "    hiddenin = (torch.zeros(1, arg.batch_size, arg.RNNSELF_SIZE, device=arg.device),\n",
    "                torch.zeros(1, arg.batch_size, arg.RNNSELF_SIZE, device=arg.device))\n",
    "    \n",
    "    return state, hiddenin\n",
    "\n",
    "\n",
    "for i_epi in range(num_trial):\n",
    "    if i_epi % (arg.full_len / arg.truncated_len) == 0:\n",
    "        state, hiddenin = get_init_variables()\n",
    "        env.reset()\n",
    "\n",
    "    agent.buffer.actorhidden0s.append(hiddenin)\n",
    "    \n",
    "    state_vectors = []; states = []; actions = []; actionlogprobs = []; rewards = []\n",
    "    \n",
    "    for t in range(arg.truncated_len):\n",
    "        action, action_logprob, hiddenout, dist = agent.select_action(state, hiddenin, \n",
    "                                                                      enable_noise=True, return_dist=True)\n",
    "        action_cpu = action.cpu().squeeze().numpy()\n",
    "        reward_shaped, reward = env.push_button(action_cpu)\n",
    "        reward_shaped = torch.tensor(reward_shaped, device=arg.device, \n",
    "                                     dtype=torch.float).reshape(1, -1, 1) * arg.REWARD_SCALE\n",
    "        reward = torch.tensor(reward, device=arg.device, dtype=torch.float).reshape(1, -1, 1)\n",
    "        # store data\n",
    "        reward_log += reward.mean()\n",
    "        reward_num += reward.sum()\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        actionlogprobs.append(action_logprob)\n",
    "        rewards.append(reward_shaped)\n",
    "        \n",
    "        # next step\n",
    "        next_obs = env.get_obs()\n",
    "        next_obs = torch.tensor(next_obs, device=arg.device, dtype=torch.float).unsqueeze(0)\n",
    "        next_state = torch.cat([next_obs, action / 3, reward, torch.ones_like(reward) * t / arg.truncated_len], dim=-1)\n",
    "            \n",
    "        # update variables\n",
    "        hiddenin = hiddenout\n",
    "        state = next_state\n",
    "        \n",
    "    states.append(state)\n",
    "    states = torch.cat(states); agent.buffer.states.append(states)\n",
    "    actions = torch.cat(actions); agent.buffer.actions.append(actions)\n",
    "    actionlogprobs = torch.cat(actionlogprobs); agent.buffer.actionlogprobs.append(actionlogprobs)\n",
    "    rewards = torch.cat(rewards); agent.buffer.rewards.append(rewards)\n",
    "    \n",
    "    \n",
    "    # update model\n",
    "    losses = agent.learn()  \n",
    "    agent.save(i_epi)\n",
    "\n",
    "    # print\n",
    "    entropy = losses[2].cpu().numpy()\n",
    "    reward_frac = (reward_num.item()) / (arg.batch_size * arg.full_len)\n",
    "\n",
    "    print(f'epi {i_epi},', f'reward_avg {reward_log.item(): .3f},', f'reward_num {reward_num.item(): .0f},',\n",
    "          f'reward_frac {reward_frac: .3f},', f'entropy {np.round(entropy.item(), 3)}')\n",
    "\n",
    "    agent.buffer.clear()\n",
    "\n",
    "    reward_log = 0\n",
    "    reward_num = 0\n",
    "    done_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa2358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c64e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
